1. What is a Neural Network?
In the nueral network combines Xs to be new features and connects the new features to output Y.
Given the input Xs and output Ys, the middle features are generated/figured out.

What is Supervised Learning?
Given input and output to train. Types: Standard NN, CNN (image data), RNN (sequence data), Hybrid

Structured Data: Each of the feature has a well defined meaning, Database data
Unstructured Data: Audio, Image, Text

Driver Behind the Rise of Deep Learning:
- Traiditional learning algorithm's performance is ceiled with large amount of data.
- Small Neural Network's performance is already better than the traditional one. 
- Larger Neural Network results in better performance.
- Data
-Computation
-Algorithms	 e.g. Sigmoid -> ReLU , make gradient descent much faster!


Label:
m = amount of labeled data


Problem: 
Binary Classification
e.g. a 64 x 64 pixels image = 3 layers (RGB) of 64 X 64 pixels = 64 X 64 X 3 = 12288(nx) input X -> y={0,1}
			X is a nx*m matrix		X.shape = (nx,m)
			Y is a 1*m matrix			Y.shape = (1,m)
			
			
6. Logistic Regression
Given x, want y_hat = P(y=1|x)
Parameter: w		b
Output: Sigmoid(w_transpose*x + b) = Sigmoid(z) = 1/(1+e^-z)			If z is large, Sigmoid(z) = 1	; If z is small, Sigmoid(z) = 0
The job is to learn w and b such that the prediction is accurate.
It turns out it would be easier to implement when just keeping b & w separate!!! (rather than treat b as x0)

7. Logistic Regression Cost Function
Given labels, want y_hat(i) = y(i)
Loss function: 
L(y_hat,y) = 1/2 * (y_hat - y)^2  <- not used in Logistic Regression (not a good choice, many local minimum)
L(y_hat,y) = -(ylog(y_hat) + (1-y)log(1-y_hat)) <- this is used
If y = 1: L(y_hat,y) = -log(y_hat) <- want log(y_hat) to be large <- want y_hat be large
If y = 0: L(y_hat,y) = log(1-y_hat) <- want log(1-y_hat) large <- want y_hat small
[There are other functions giving similar characteristics, but this one is the best, explained in later chapter.]
Cost function: J(w,b) = 1/m * Sum(L(y_hat(i),y(i))) = -1/m * Sum[y(i)log(y_hat(i)) + (1-y(i))log(1-y_hat(i))]

 Loss function -> single training sample
 Cost function -> parameter (w,b)
-----------------------------------------------------------------------------------------
 8. Gradient Descent
 We use that cost function (logistic regression) because it is convex!(only 1 global minimum)
 
w := w - alpha* dJ(w)/dw  [when coding, use {dw} to represent the derivative term]
alpha: learning rate
If w > global minimum , w := smaller w
If w < global minimum , w := larger w
 
b := b - alpha* dJ(w,b)/db [when coding, use {db} to represent the derivative term]
[Rule in calculus: if only one parameter, use {d}. otherwise, use the curved {d}]
-----------------------------------------------------------------------------------------
9,10. Derivatices

-----------------------------------------------------------------------------------------
11,12. Computation Graph

J(a,b,c) = 3(a+bc)
u = bc
v = a+u
J = 3v

Comes in handy when you wanna minimize parameters
Backward Propagation!!!

dJ/dv -> dv/da -> dJ/da
[in coding, d(finalOutput)/d(var) = dvar]
-----------------------------------------------------------------------------------------
13. Logistic Regression Derivatives

y_hat = a = sigmoid(z)
"da" = dL(a,y)/da = -y/a + (1-y)/(1-a)
"dz" = dL/dz = a-y = dL/da * da/dz
da/dz = a(1-a)
dL/dw1 = "dw" = x1 * dz
-----------------------------------------------------------------------------------------
14. Gradient Descent on m Training Examples

J=0, dw1=0, dw2=0, db=0

For i = 1 to m,

z = wTx +b
a = sigmoid(z)
J += [yloga + (1-y)log(1-a)]
dz = a - y
dw1 += x1 * dz
dw2 += x1 * dz
J/=m, dw1/=m, dw2/=m

w1 := w1 - alpha * dw1
w2 := w2 - alpha * dw2
b := b - alpha * db

Disadvantages: 
	two for loops ( 1 to m ; for all features) -> inefficient

Solution: 
	Vectorization technique (getting rid for loop)
-----------------------------------------------------------------------------------------
15,16. Vectorization

	z = np.dot(w,x) + b

CPU & GPU -> SIMD - single instruction mulitple data (parallelism)

Non-vectorized:
	u = np.zeros((n,1))		
	 for i in range(n):
	  u[i] = math.exp(v[i])   
	  
Vectorized:
	import numpy as np
	u = np.exp(v)
	np.log(v)
	np.abs(v)
	np.maximum(v,0)
	
Whenever you attempt to write a for loop, take a look at whether have a vectorized form.

-----------------------------------------------------------------------------------------
17. Vectorizing Logistic Regression
	
	z = np.dot(wT, X) + b      <-- b : broadcasting in python

-----------------------------------------------------------------------------------------
18. Vectorizing Logistic Regression's Gradient Computation

dz(i) = a(i) - y(i)
dZ = [dz(1), dz(2) ... dz(m)]
A = [a(1), ... a(m)]
Y = [y(1)...y(m)]

dZ = A - Y

db = 1/m np.sum(dZ)
dw = 1/m XdZ_T

Final:	
1.	Z = np.dot(wT,X) + b
2.	dw = 1/m XdZ_T
3.	db = 1/m np.sum(dZ)
4.	w := w - alpha* dw
5.	b := b - alpha* dw

-----------------------------------------------------------------------------------------
19. Broadcasting in Python 

	cal = A.sum(axis=0) [axis=0 -> vertically; axis=1 -> horizontally]
	100*A/(cal.reshape(1,4)) [the reshape operation is cheap]
	
	[1,2,3,4] + 100 ==> [101,102,103,104]
	(m,n) + (1,n) ==> (1,n) Copy to (m,n)
	(m,n) + (m,1) ==> (m,1) Copy to (m,n)
	

-----------------------------------------------------------------------------------------
20. Python-Numpy

a = np.random.randn(5)   [a shape = (5,)] <- Don't use "rank 1 array"
a = np.random.randn(5,1) -> a.shape = (5,1)
a = np.random.randn(1,5) -> a.shape = (1,5)
assert(a.shape == (5,1))
a = a.reshape((5,1))

-----------------------------------------------------------------------------------------
21. Jupyter-iPython
-----------------------------------------------------------------------------------------
22. Logistic Regression Cost Function Explanation [?_?]

Interpret y_hat = P(y=1|x)
	If y=1 : P(y|x) = y_hat
	If y=0 : P(y|x) = 1-y_hat
	P(y|x) = y_hat^y * (1-y_hat)^(1-y)
	log(P(y|x)) = -L        [maximum likelihood estimation]

-----------------------------------------------------------------------------------------
23,24,25,26,27. Neural Network Overview

Input layer: a[0] = X   / layer 0 
Hidden layer: not observed  a[1]
Output layer: a[2] = y_hat

[Don't count the input layer]

A node: calculate z and a = sigmoid(z)  

** W = [wT...] **

Vectorize
-----------------------------------------------------------------------------------------
28. Activation Functions ?_?

	tanh(z) = (e^z - e^-z)/(e^z + e^-z) (shifted version of sigmoid function) 
	[almost always better than sigmoid function] 
	[except for the output layer]
	
	ReLU(z) = max(0,z)
	Leaky ReLU(z) = max(0.01z,z)
	
Rules of Thumb:
1. If output is 0 or 1, use sigmoid at output layer
2. ReLU (solve the slow gradient descent problem; much faster)/ Leaky ReLU
3. Never use sigmoid 
-----------------------------------------------------------------------------------------
29. Why Non-Linear Activation Function?

	Why not set a = z?
	
If we use a linear activation function, the result is just a linear function.
-> more or less useless
-> you can combine all the layers into one layer
-> only use linear activation function in output layer makes sense
-----------------------------------------------------------------------------------------
30. Derivatives of Activation Functions

Sigmoid derivative: g'(z) = g(z)(1-g(z)) = a(1-a)
Tanh derivative: g'(z) = 1 - g(z)^2 = 1 - a^2
ReLU derivative: g'(z) = 0 if z <0
		 g'(z) = 1 if z >=0
		 undefined if z = 0
		
Leaky ReLU derivative:  g'(z) = 0.01 if z <0
		 	g'(z) = 1 if z >=0
		 	
-----------------------------------------------------------------------------------------
31. Gradient Descent for Neural Networks

Parameters: 
w[1]: (n[1], n[0])
b[1]: (n[1],1)

Gradient descent:
 Repeat {
 		compute predicts (y_hat, i=1..m)
		dw[1] = dJ/dw[1], db[1]= dJ/db[1],...
		W[1] := W[1] - alpha*dW[1]
		b[1] := b[1] - alpha*db[1]
		...
	}

Forward propagation:
Z[1] = W[1]X + b[1]
A[1] = g[1](Z[1])
Z[2] = W[2]A[1] + b[2]
A[2] = g[2](Z[2]) = Simoid(Z[2])

Back propagation:
dZ[2] = A[2] - Y	Y = [y(1)...y(m)]
dW[2] = 1/m dZ[2]*A[1]_T
db[2] = 1/m np.sum(dZ[2],axis = 1, keepdims = True)

dZ[1] = W[2]_T dZ[2] * g[1]')(Z[1]) {both are (n(1),m) element-wise *}
dW[1] = 1/m dZ[1]X_T
db[1] = 1/m np.sum(dZ[1], axis = 1, keepdims = True)

Intuition: see logistice regression 

-----------------------------------------------------------------------------------------
33. Random Initialization of Weights

What happens if initializing the weights to be zero?
	- Start off computing the same function/ same influence
	- Symmetric (no matter how long we train it)
	- Useless
	
Solution:
	Random initialization 
	W[1] = np.random.randn((2,2)) * 0.01 {if w is very big, it may end up landing on flat part of the curve -> slow training}
	b[1] = np.zerp((2,1)) {- no symmetric problem}
	...
	b[2] = 0
-----------------------------------------------------------------------------------------
34,35,36,37. Deep L-layer Neural Network Forward Propagation

We cannot get rid of the for-loop for layers

Representation:
1. recognize edges -> face recognization
2. early layers -> simple features e.g.edges-> later layers -> complex things e.g.face

Circuit Theory:
	There are functions you can compute with a "small" L-layer deep neural network that shallower networks require exponentially more hidden units to compute.
	[functions are much easier to compute with deep network]

-----------------------------------------------------------------------------------------
38,39. Building Blocks of Deep Neural Network; Forward Propagation for Layer L

Forward: Input a[l-1], output a[l]
z[l]: w[l]a[l-1]+b[l] 	cache z[l]
a[l]: g[l](z[l])

Backward: Input da[l], output da[l-1]
	cache(z[l])	dw[l]& db[l]
	

-----------------------------------------------------------------------------------------
40. Parameters Vs. Hyperparameters

Parameters: W[1], b[1] ...

Hyperparameters: parameters that control "W and b"
- Learning rate (alpha)
- iterations
- hidden laer L
- hidden units n[1], n[2]
- choice of activation functon

- momentum
- minibatch size
- regularization

[Need to apply deep learning in a very empirical process so far (try out a lot of things)]
-----------------------------------------------------------------------------------------
41. Brain and Deep Learning
-----------------------------------------------------------------------------------------
42. Train/Dev/Test sets

ML is a highly iterative process 		Idea -> Code -> Experiment -> Idea ...

Data { training set | Hold-out cross validation; Development set "dev"| test}

Previous era: 70%/30% or 60%/20%/20%

Modern big data era: 98%/1%/1%  (since we don't need so many data for dev and test ==> efficiency)

[Mismatched train/test distribution (cost)] - make sure dev and test sets come from same distribution **
[Not having a test set might be okay] - only dev set
-----------------------------------------------------------------------------------------
43. Bias/Variance

[depends on the optimal (Bayes) error]
high bias -> underfitting
high variance -> overfitting
-----------------------------------------------------------------------------------------
44. Basic "Recipe" of Machine Learning

- High bias? [Training data performance]--> Bigger network; Train longer; NN architecture search
- High variance? [Dev set performance] --> More data; Regularization; NN architecture search

"Bias variance tradeoff" ==> no longer a must 
-----------------------------------------------------------------------------------------
45. Regularization (reduce overfitting) ?_?

*L2 regularization:
J(w,b)= 1/m Sum(L(y_hat,y)) + lambda/2m ||w||_2^2
||w||_2^2 = Sum(wj^2) = w_T w

[we usually omit "b" because most of the parameters are in "w"]

*L1 regularization:

lambda/2m Sum(|w|) =  lambda/2m ||w||_1    ==> more sparse (contain a lot of zeros)

Lamda: regularization parameter



-----------------------------------------------------------------------------------------
46. Why Regularization reduces Overfitting?

Intuition: 
	if set lambda to be very big -> make w close to zero -> more simple/smaller network -> higher bias

-----------------------------------------------------------------------------------------
47,48. Dropout regularization / Why it works?

[faster test time]
Inverted dropout:
-keep-prob: the probability of being dropped

Intuition:
	Can't rely on any one feature. so have to spread out weights -> Shrink weights
	
Application: Computer vision

Downsize: J is not well defined
-----------------------------------------------------------------------------------------
49. Other Regularization Methods

(a)[give you more data to reduce overfitting]
- Flip horizontally
- Rotate 
- Distortion 

(b) Early stopping
- w is getting bigger and bigger along training
Downsize: cannot solve the two problems [optimize J & not overfit] independently
-----------------------------------------------------------------------------------------
50. Normalizing Input

(a) Substract mean => get 0 mean
	x := x-x_bar

(b) Normalize variance ==> get sigma = 1
	sigma^2 = 1/m Sum(x^2)  [element-wise]
	x /= sigma
similar scale -> easier and faster to optimize J  e.g. gradient descent/ overshoot, need small alpha

-----------------------------------------------------------------------------------------
52. Vanishing/Exploding Gradients

[numerical problems]
W > I  : Explode
W < I : Vanish
-----------------------------------------------------------------------------------------
53. Weight Initialization for Deep Network

The larger n -> smaller w [in order not to explode]
Var(wi) = 1/n
W[l] = np.random.randn(shape)*np.sqrt(2/n[l-1])     <- 2/n[l-1] is better for ReLU


tanh: np.sqrt(1/n[l-1])

Other:
np.sqrt(2/(n[l-1]+n[l]))


-----------------------------------------------------------------------------------------
53. Numerical Approximation of Gradients

- Check backprop is correct

- instead of using the triangle between x and x + epsilon, use triangle between x-epsilon and x+epsilon (much more accurate)

The approx. error is very small.
-----------------------------------------------------------------------------------------
54,55. Gradient Checking

(a) Take W and b into a big vector theta
(b) Take dW and db into a big vector dtheta

dtheta_approx.[i] = [J(theta[1]...theta[i]+epsilon ...) -  J(theta[1]...theta[i]-epsilon ...)]/2epsilon

(c) Check ||dtheta_approx - dtheta||/(||dtheta_approx||_2 + ||dtheta||2):
= 10^-7 > Great
= 10^-5 
= 10^-3 > Wrong


[Don't use in training - only to debug]
[If fails -> look at components to try to identify bug]
[Remember regularization]
[Doesn't work with dropout - make keep.prob = 1.0]
[Run at random initialization] - barely happen -> when small value correct/ large value incorect
-----------------------------------------------------------------------------------------
56,57. Mini Batch Gradient Descent

Batch -> previous version
Mini Batch -> process mini batch instead of entire batch

Use vectorization to process all the mini batches

"1 epoch" : single pass through training set [for mini batch, 1 epoch = many training steps]

Pros: Much faster

Extreme case: 
mini-batch size = m   -> Batch Gradient Descent		[relatively low noise & large step, take too much time per iteration]
mini-batch size = 1   -> Stochastic Gradient Descent [extremely noisy (can be reduced by using small learning rate), never converge, lose all the speed up from vectorization]
mini-batch size = 1 < size < m [fastest: vectorization; make progress without processing entire training set]

Guideline for choosing mini-batch size:
(a) if small training set -> use batch gradient descent
(b) typical mini-batch sizes: [power of 2] 64, 128, 256, 512
(c) make sure minibatch fit in CPU/GPU memory

-----------------------------------------------------------------------------------------
58. Exponentially Weighted Averages

vt = beta*v[t-1] + (1-beta)*theta[t]

Moving average [beta=0.9 -> average of 10days (to 1/3)]:
v0 = 0
v1 = 0.9v0 + 0.1theta[1]
v2= 0.9v1 + 0.1theta[2]
...
vt = 0.9vt-1 + 0.1theta[t]

[computational expensive & memory intensive]
[1 line of code]
-----------------------------------------------------------------------------------------
60. Bias Correction in Exponentially Weighted Average

initial value is very slow

V[t]/(1-beta^t)   [when t is large -> no difference]
-----------------------------------------------------------------------------------------
61. Gradient Descent with Momentum

Using larger learning rate -> overshoot

Momentum:
V(dW) = beta* V(dW) + (1-beta)*dW

0.9 is most robust now
(in practice no one bother bias correction) -> after 10 iteration -> already warm up

W := W - alpha*V[dW]
-----------------------------------------------------------------------------------------
62. RMSprop (root mean squared prop)

[exponential weighted average]
S(dW) = beta* S(dW) + (1-beta)*d(W^2) <--small 
S(db) = beta* S(db) + (1-beta)*d(b^2) <--large 
W := W - alpha*(dW/sqrt(S[dW]))
b := b - alpha*(db/sqrt(S[db]))

Pro: can use larger alpha
-----------------------------------------------------------------------------------------
63. Adam Optimization Algorithm

momentum + RMSprop + bias correction

W := W - alpha*V[dW]/[sqrt(S[dW])+epsilon]

Advantage: effective

alpha: needs to be tune
beta1: 0.9 (dw)
beta2: 0.999 (dw^2)
epsilon: 10^-8
-----------------------------------------------------------------------------------------
64. Learning Rate Decay

Problem:
- Never converge, if reduce alpha -> slow

Solution:
alpha = alpha_0/(1+ decay_rate*epochNum)
alpha = alpha_0* 0.95^epochNum - exponentially decay
alpha = k*alpha_0/sqrt(epochNum) - discrete staircase
Manual decay

-----------------------------------------------------------------------------------------
65. The Problem of Local Optima 
Since there are lots of dimension, local optima are not really a problem. The chance of getting a saddle point is extremely low.

Problem: plateaus -> make learning slow

-----------------------------------------------------------------------------------------
66,67,68. Tunning Process ; Right Scale for Hyperparameters ?_?

***alpha
**momentum term beta~ 0.9
**mini-batch size
**#hidden units
*#layers
*learning rate decay

Try random values: Don't use a grid
Coarse to fine (also frequently used)




-----------------------------------------------------------------------------------------
73,74. Softmax Regression

More than 2 classes

Activation function:
	t = e^z[l]
	vector -> a[l] = t/Sum(t)  -> vector
	
Loss function: L(y^hat,y) = -Sum(ylog(y_hat))

-----------------------------------------------------------------------------------------
78. Orthogonalization

Control things independently, separate effects

Assumptions:
(a) Fit training set well on cost function 		[Bigger network, Adam ...]   	(X_X)early stopping affects a&b
(b) Fit dev set well on cost function			[Regularization, Bigger training set...]
(c) Fit test set well on cost function			[Bigger dev set]
(d) Performs well in real world				[Change dev set or cost function]


-----------------------------------------------------------------------------------------
79. Single Number Evaluation Metric

Precision: how many examples recognized as cat, what % actually are cats?
Recall: What % of actual cats are correctly recognized
[often a trade-off]
F1 score = "Average" of P and R = 2/(1/P + 1/R)

Classifer	Precision	Recall		F1 Score
A		95%		90%		92.4%
B		98%		85%		91.0%

Dev + Single Number Evaluation Metric -> speed up iterative process

-----------------------------------------------------------------------------------------
80. Satisfying and Optimizing Metrics

Maximizing accuracy subject to runningTime <= 100ms

N metrics: 1 optimizing, N-1 satisfying

-----------------------------------------------------------------------------------------
84. Why human-level performance?

(a)Get labeled data from human
(b) Gain insight from manual error analysis: Why did a person get this right?
(c) Better analysis of bias/variance
-----------------------------------------------------------------------------------------
85. Avoidable Bias (difference between approx. Bayes error and training error)

Human-level error as a proxy for Bayes error.

dev error <-> training error  	: variance
-----------------------------------------------------------------------------------------
87. Surpassing Human-Level Performance

(a) Online advertising
(b) Product recommendations
(c) Logistics (predicting transit ttime)
(d) Loan approvals

- Structural data
- Not natural perception
- Lots of data

- Speech recognition
- Some image recognition
- Some medical
-----------------------------------------------------------------------------------------
90. Carrying Out Error Analysis
-----------------------------------------------------------------------------------------
95. Transfer Learning

If data are not big:
delete the last layer -> swap it to the new data

If data are enough: 
retrain 
the original model is called pre-training

[Can train more than 1 layer]

When transfer learning makes sense:
- Task A and B have the same input x
- Have a lot more data for Task A than Task B
- Low level features from A could be helpful for B
-----------------------------------------------------------------------------------------
96. Multi-Task Learning

train all the features simultaneously is more effective than training separately
Even some labels are missing, it also works

When muli-task learning makes sense:
- Training on set of tasks that could benefit from haveing shared lower-level features
- Usually: Amount of data you have for each task is quite similar
- Can train a big enough NN to do well on all the tasks
-----------------------------------------------------------------------------------------
97,98. End-to-End Deep Learning

Data vs Hand-design (inject knowledge)

Pros: 
- Let the data speak	x->y
- Less hand-designing of components needed

Cons:
- May need large amount of data
- Excludes potentially useful hand-designed components (which could be very useful)

Key question:	Do you have sufficient data to learn a function of the complexity needed to map x -> y
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
