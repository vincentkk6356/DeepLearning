1. What is a Neural Network?
In the nueral network combines Xs to be new features and connects the new features to output Y.
Given the input Xs and output Ys, the middle features are generated/figured out.

What is Supervised Learning?
Given input and output to train. Types: Standard NN, CNN (image data), RNN (sequence data), Hybrid

Structured Data: Each of the feature has a well defined meaning, Database data
Unstructured Data: Audio, Image, Text

Driver Behind the Rise of Deep Learning:
- Traiditional learning algorithm's performance is ceiled with large amount of data.
- Small Neural Network's performance is already better than the traditional one. 
- Larger Neural Network results in better performance.
- Data
-Computation
-Algorithms	 e.g. Sigmoid -> ReLU , make gradient descent much faster!


Label:
m = amount of labeled data


Problem: 
Binary Classification
e.g. a 64 x 64 pixels image = 3 layers (RGB) of 64 X 64 pixels = 64 X 64 X 3 = 12288(nx) input X -> y={0,1}
			X is a nx*m matrix		X.shape = (nx,m)
			Y is a 1*m matrix			Y.shape = (1,m)
			
			
6. Logistic Regression
Given x, want y_hat = P(y=1|x)
Parameter: w		b
Output: Sigmoid(w_transpose*x + b) = Sigmoid(z) = 1/(1+e^-z)			If z is large, Sigmoid(z) = 1	; If z is small, Sigmoid(z) = 0
The job is to learn w and b such that the prediction is accurate.
It turns out it would be easier to implement when just keeping b & w separate!!! (rather than treat b as x0)

7. Logistic Regression Cost Function
Given labels, want y_hat(i) = y(i)
Loss function: 
L(y_hat,y) = 1/2 * (y_hat - y)^2  <- not used in Logistic Regression (not a good choice, many local minimum)
L(y_hat,y) = -(ylog(y_hat) + (1-y)log(1-y_hat)) <- this is used
If y = 1: L(y_hat,y) = -log(y_hat) <- want log(y_hat) to be large <- want y_hat be large
If y = 0: L(y_hat,y) = log(1-y_hat) <- want log(1-y_hat) large <- want y_hat small
[There are other functions giving similar characteristics, but this one is the best, explained in later chapter.]
Cost function: J(w,b) = 1/m * Sum(L(y_hat(i),y(i))) = -1/m * Sum[y(i)log(y_hat(i)) + (1-y(i))log(1-y_hat(i))]

 Loss function -> single training sample
 Cost function -> parameter (w,b)
-----------------------------------------------------------------------------------------
 8. Gradient Descent
 We use that cost function (logistic regression) because it is convex!(only 1 global minimum)
 
w := w - alpha* dJ(w)/dw  [when coding, use {dw} to represent the derivative term]
alpha: learning rate
If w > global minimum , w := smaller w
If w < global minimum , w := larger w
 
b := b - alpha* dJ(w,b)/db [when coding, use {db} to represent the derivative term]
[Rule in calculus: if only one parameter, use {d}. otherwise, use the curved {d}]
-----------------------------------------------------------------------------------------
9,10. Derivatices

-----------------------------------------------------------------------------------------
11,12. Computation Graph

J(a,b,c) = 3(a+bc)
u = bc
v = a+u
J = 3v

Comes in handy when you wanna minimize parameters
Backward Propagation!!!

dJ/dv -> dv/da -> dJ/da
[in coding, d(finalOutput)/d(var) = dvar]
-----------------------------------------------------------------------------------------
13. Logistic Regression Derivatives

y_hat = a = sigmoid(z)
"da" = dL(a,y)/da = -y/a + (1-y)/(1-a)
"dz" = dL/dz = a-y = dL/da * da/dz
da/dz = a(1-a)
dL/dw1 = "dw" = x1 * dz
-----------------------------------------------------------------------------------------
14. Gradient Descent on m Training Examples

J=0, dw1=0, dw2=0, db=0

For i = 1 to m,

z = wTx +b
a = sigmoid(z)
J += [yloga + (1-y)log(1-a)]
dz = a - y
dw1 += x1 * dz
dw2 += x1 * dz
J/=m, dw1/=m, dw2/=m

w1 := w1 - alpha * dw1
w2 := w2 - alpha * dw2
b := b - alpha * db

Disadvantages: 
	two for loops ( 1 to m ; for all features) -> inefficient

Solution: 
	Vectorization technique (getting rid for loop)
-----------------------------------------------------------------------------------------
15,16. Vectorization

	z = np.dot(w,x) + b

CPU & GPU -> SIMD - single instruction mulitple data (parallelism)

Non-vectorized:
	u = np.zeros((n,1))		
	 for i in range(n):
	  u[i] = math.exp(v[i])   
	  
Vectorized:
	import numpy as np
	u = np.exp(v)
	np.log(v)
	np.abs(v)
	np.maximum(v,0)
	
Whenever you attempt to write a for loop, take a look at whether have a vectorized form.

-----------------------------------------------------------------------------------------
17. Vectorizing Logistic Regression
	
	z = np.dot(wT, X) + b      <-- b : broadcasting in python

-----------------------------------------------------------------------------------------
18. Vectorizing Logistic Regression's Gradient Computation

dz(i) = a(i) - y(i)
dZ = [dz(1), dz(2) ... dz(m)]
A = [a(1), ... a(m)]
Y = [y(1)...y(m)]

dZ = A - Y

db = 1/m np.sum(dZ)
dw = 1/m XdZ_T

Final:	
1.	Z = np.dot(wT,X) + b
2.	dw = 1/m XdZ_T
3.	db = 1/m np.sum(dZ)
4.	w := w - alpha* dw
5.	b := b - alpha* dw

-----------------------------------------------------------------------------------------
19. Broadcasting in Python 

	cal = A.sum(axis=0) [axis=0 -> vertically; axis=1 -> horizontally]
	100*A/(cal.reshape(1,4)) [the reshape operation is cheap]
	
	[1,2,3,4] + 100 ==> [101,102,103,104]
	(m,n) + (1,n) ==> (1,n) Copy to (m,n)
	(m,n) + (m,1) ==> (m,1) Copy to (m,n)
	

-----------------------------------------------------------------------------------------
20. Python-Numpy

a = np.random.randn(5)   [a shape = (5,)] <- Don't use "rank 1 array"
a = np.random.randn(5,1) -> a.shape = (5,1)
a = np.random.randn(1,5) -> a.shape = (1,5)
assert(a.shape == (5,1))
a = a.reshape((5,1))

-----------------------------------------------------------------------------------------
21. Jupyter-iPython
-----------------------------------------------------------------------------------------
22. Logistic Regression Cost Function Explanation [?_?]

Interpret y_hat = P(y=1|x)
	If y=1 : P(y|x) = y_hat
	If y=0 : P(y|x) = 1-y_hat
	P(y|x) = y_hat^y * (1-y_hat)^(1-y)
	log(P(y|x)) = -L        [maximum likelihood estimation]

-----------------------------------------------------------------------------------------
23,24,25,26,27. Neural Network Overview

Input layer: a[0] = X   / layer 0 
Hidden layer: not observed  a[1]
Output layer: a[2] = y_hat

[Don't count the input layer]

A node: calculate z and a = sigmoid(z)  

** W = [wT...] **

Vectorize
-----------------------------------------------------------------------------------------
28. Activation Functions ?_?

	tanh(z) = (e^z - e^-z)/(e^z + e^-z) (shifted version of sigmoid function) 
	[almost always better than sigmoid function] 
	[except for the output layer]
	
	ReLU(z) = max(0,z)
	Leaky ReLU(z) = max(0.01z,z)
	
Rules of Thumb:
1. If output is 0 or 1, use sigmoid at output layer
2. ReLU (solve the slow gradient descent problem; much faster)/ Leaky ReLU
3. Never use sigmoid 
-----------------------------------------------------------------------------------------
29. Why Non-Linear Activation Function?

	Why not set a = z?
	
If we use a linear activation function, the result is just a linear function.
-> more or less useless
-> you can combine all the layers into one layer
-> only use linear activation function in output layer makes sense
-----------------------------------------------------------------------------------------
30. Derivatives of Activation Functions

Sigmoid derivative: g'(z) = g(z)(1-g(z)) = a(1-a)
Tanh derivative: g'(z) = 1 - g(z)^2 = 1 - a^2
ReLU derivative: g'(z) = 0 if z <0
		 g'(z) = 1 if z >=0
		 undefined if z = 0
		
Leaky ReLU derivative:  g'(z) = 0.01 if z <0
		 	g'(z) = 1 if z >=0
		 	
-----------------------------------------------------------------------------------------
31. Gradient Descent for Neural Networks

Parameters: 
w[1]: (n[1], n[0])
b[1]: (n[1],1)

Gradient descent:
 Repeat {
 		compute predicts (y_hat, i=1..m)
		dw[1] = dJ/dw[1], db[1]= dJ/db[1],...
		W[1] := W[1] - alpha*dW[1]
		b[1] := b[1] - alpha*db[1]
		...
	}

Forward propagation:
Z[1] = W[1]X + b[1]
A[1] = g[1](Z[1])
Z[2] = W[2]A[1] + b[2]
A[2] = g[2](Z[2]) = Simoid(Z[2])

Back propagation:
dZ[2] = A[2] - Y	Y = [y(1)...y(m)]
dW[2] = 1/m dZ[2]*A[1]_T
db[2] = 1/m np.sum(dZ[2],axis = 1, keepdims = True)

dZ[1] = W[2]_T dZ[2] * g[1]')(Z[1]) {both are (n(1),m) element-wise *}
dW[1] = 1/m dZ[1]X_T
db[1] = 1/m np.sum(dZ[1], axis = 1, keepdims = True)

Intuition: see logistice regression 

-----------------------------------------------------------------------------------------
33. Random Initialization of Weights

What happens if initializing the weights to be zero?
	- Start off computing the same function/ same influence
	- Symmetric (no matter how long we train it)
	- Useless
	
Solution:
	Random initialization 
	W[1] = np.random.randn((2,2)) * 0.01 {if w is very big, it may end up landing on flat part of the curve -> slow training}
	b[1] = np.zerp((2,1)) {- no symmetric problem}
	...
	b[2] = 0
-----------------------------------------------------------------------------------------
34,35,36,37. Deep L-layer Neural Network Forward Propagation

We cannot get rid of the for-loop for layers

Representation:
1. recognize edges -> face recognization
2. early layers -> simple features e.g.edges-> later layers -> complex things e.g.face

Circuit Theory:
	There are functions you can compute with a "small" L-layer deep neural network that shallower networks require exponentially more hidden units to compute.
	[functions are much easier to compute with deep network]

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
