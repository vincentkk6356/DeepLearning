1. What is a Neural Network?
In the nueral network combines Xs to be new features and connects the new features to output Y.
Given the input Xs and output Ys, the middle features are generated/figured out.

What is Supervised Learning?
Given input and output to train. Types: Standard NN, CNN (image data), RNN (sequence data), Hybrid

Structured Data: Each of the feature has a well defined meaning, Database data
Unstructured Data: Audio, Image, Text

Driver Behind the Rise of Deep Learning:
- Traiditional learning algorithm's performance is ceiled with large amount of data.
- Small Neural Network's performance is already better than the traditional one. 
- Larger Neural Network results in better performance.
- Data
-Computation
-Algorithms	 e.g. Sigmoid -> ReLU , make gradient descent much faster!


Label:
m = amount of labeled data


Problem: 
Binary Classification
e.g. a 64 x 64 pixels image = 3 layers (RGB) of 64 X 64 pixels = 64 X 64 X 3 = 12288(nx) input X -> y={0,1}
			X is a nx*m matrix		X.shape = (nx,m)
			Y is a 1*m matrix			Y.shape = (1,m)
			
			
6. Logistic Regression
Given x, want y_hat = P(y=1|x)
Parameter: w		b
Output: Sigmoid(w_transpose*x + b) = Sigmoid(z) = 1/(1+e^-z)			If z is large, Sigmoid(z) = 1	; If z is small, Sigmoid(z) = 0
The job is to learn w and b such that the prediction is accurate.
It turns out it would be easier to implement when just keeping b & w separate!!! (rather than treat b as x0)

7. Logistic Regression Cost Function
Given labels, want y_hat(i) = y(i)
Loss function: 
L(y_hat,y) = 1/2 * (y_hat - y)^2  <- not used in Logistic Regression (not a good choice, many local minimum)
L(y_hat,y) = -(ylog(y_hat) + (1-y)log(1-y_hat)) <- this is used
If y = 1: L(y_hat,y) = -log(y_hat) <- want log(y_hat) to be large <- want y_hat be large
If y = 0: L(y_hat,y) = log(1-y_hat) <- want log(1-y_hat) large <- want y_hat small
[There are other functions giving similar characteristics, but this one is the best, explained in later chapter.]
Cost function: J(w,b) = 1/m * Sum(L(y_hat(i),y(i))) = -1/m * Sum[y(i)log(y_hat(i)) + (1-y(i))log(1-y_hat(i))]

 Loss function -> single training sample
 Cost function -> parameter (w,b)
-----------------------------------------------------------------------------------------
 8. Gradient Descent
 We use that cost function (logistic regression) because it is convex!(only 1 global minimum)
 
w := w - alpha* dJ(w)/dw  [when coding, use {dw} to represent the derivative term]
alpha: learning rate
If w > global minimum , w := smaller w
If w < global minimum , w := larger w
 
b := b - alpha* dJ(w,b)/db [when coding, use {db} to represent the derivative term]
[Rule in calculus: if only one parameter, use {d}. otherwise, use the curved {d}]
-----------------------------------------------------------------------------------------
9,10. Derivatices

-----------------------------------------------------------------------------------------
11,12. Computation Graph

J(a,b,c) = 3(a+bc)
u = bc
v = a+u
J = 3v

Comes in handy when you wanna minimize parameters
Backward Propagation!!!

dJ/dv -> dv/da -> dJ/da
[in coding, d(finalOutput)/d(var) = dvar]
-----------------------------------------------------------------------------------------
13. Logistic Regression Derivatives

y_hat = a = sigmoid(z)
"da" = dL(a,y)/da = -y/a + (1-y)/(1-a)
"dz" = dL/dz = a-y = dL/da * da/dz
da/dz = a(1-a)
dL/dw1 = "dw" = x1 * dz
-----------------------------------------------------------------------------------------
14. Gradient Descent on m Training Examples

J=0, dw1=0, dw2=0, db=0

For i = 1 to m,

z = wTx +b
a = sigmoid(z)
J += [yloga + (1-y)log(1-a)]
dz = a - y
dw1 += x1 * dz
dw2 += x1 * dz
J/=m, dw1/=m, dw2/=m

w1 := w1 - alpha * dw1
w2 := w2 - alpha * dw2
b := b - alpha * db

Disadvantages: 
	two for loops ( 1 to m ; for all features) -> inefficient

Solution: 
	Vectorization technique (getting rid for loop)
-----------------------------------------------------------------------------------------
15,16. Vectorization

	z = np.dot(w,x) + b

CPU & GPU -> SIMD - single instruction mulitple data (parallelism)

Non-vectorized:
	u = np.zeros((n,1))		
	 for i in range(n):
	  u[i] = math.exp(v[i])   
	  
Vectorized:
	import numpy as np
	u = np.exp(v)
	np.log(v)
	np.abs(v)
	np.maximum(v,0)
	
Whenever you attempt to write a for loop, take a look at whether have a vectorized form.

-----------------------------------------------------------------------------------------
17. Vectorizing Logistic Regression
	
	z = np.dot(wT, X) + b      <-- b : broadcasting in python

-----------------------------------------------------------------------------------------
18. Vectorizing Logistic Regression's Gradient Computation

dz(i) = a(i) - y(i)
dZ = [dz(1), dz(2) ... dz(m)]
A = [a(1), ... a(m)]
Y = [y(1)...y(m)]

dZ = A - Y

db = 1/m np.sum(dZ)
dw = 1/m XdZ_T

Final:	
1.	Z = np.dot(wT,X) + b
2.	dw = 1/m XdZ_T
3.	db = 1/m np.sum(dZ)
4.	w := w - alpha* dw
5.	b := b - alpha* dw

-----------------------------------------------------------------------------------------
19. Broadcasting in Python 

	cal = A.sum(axis=0) [axis=0 -> vertically; axis=1 -> horizontally]
	100*A/(cal.reshape(1,4)) [the reshape operation is cheap]
	
	[1,2,3,4] + 100 ==> [101,102,103,104]
	(m,n) + (1,n) ==> (1,n) Copy to (m,n)
	(m,n) + (m,1) ==> (m,1) Copy to (m,n)
	

-----------------------------------------------------------------------------------------
20. Python-Numpy

a = np.random.randn(5)   [a shape = (5,)] <- Don't use "rank 1 array"
a = np.random.randn(5,1) -> a.shape = (5,1)
a = np.random.randn(1,5) -> a.shape = (1,5)
assert(a.shape == (5,1))
a = a.reshape((5,1))

-----------------------------------------------------------------------------------------
21. Jupyter-iPython
-----------------------------------------------------------------------------------------
22. Logistic Regression Cost Function Explanation [?_?]

Interpret y_hat = P(y=1|x)
	If y=1 : P(y|x) = y_hat
	If y=0 : P(y|x) = 1-y_hat
	P(y|x) = y_hat^y * (1-y_hat)^(1-y)
	log(P(y|x)) = -L        [maximum likelihood estimation]

-----------------------------------------------------------------------------------------
23,24,25,26,27. Neural Network Overview

Input layer: a[0] = X   / layer 0 
Hidden layer: not observed  a[1]
Output layer: a[2] = y_hat

[Don't count the input layer]

A node: calculate z and a = sigmoid(z)  

** W = [wT...] **

Vectorize
-----------------------------------------------------------------------------------------
28. Activation Functions ?_?

	tanh(z) = (e^z - e^-z)/(e^z + e^-z) (shifted version of sigmoid function) 
	[almost always better than sigmoid function] 
	[except for the output layer]
	
	ReLU(z) = max(0,z)

Rules of Thumb:

1. If output is 0 or 1, use sigmoid at output layer
2. ReLU (solve the slow gradient descent problem; much faster)/ Leaky ReLU
3. Never use sigmoid 
4.


-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
